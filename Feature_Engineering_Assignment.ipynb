{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1.What is a parameter?**\n",
        "Ans.In feature engineering, a parameter refers to the values or settings used to transform raw data into meaningful features that improve a machine learning model's performance. These parameters are typically set during preprocessing or feature extraction and play a crucial role in shaping how a model interprets data.\n",
        "\n",
        "\n",
        "\n",
        "**2.What is correlation?\n",
        "What does negative correlation mean?**\n",
        "Ans.Correlation is a statistical measure that describes the relationship between two variables—how one variable changes in relation to another. It is often quantified using the correlation coefficient, which ranges from -1 to 1:\n",
        "- Positive correlation (closer to +1) → When one variable increases, the other tends to increase as well.\n",
        "- Negative correlation (closer to -1) → When one variable increases, the other tends to decrease.\n",
        "- No correlation (around 0) → No consistent relationship between the variables.\n",
        "\n",
        "\n",
        "\n",
        "**3.Define Machine Learning. What are the main components in Machine Learning?**\n",
        "Ans.Definition of Machine Learning\n",
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to learn patterns from data and improve performance on a given task without explicit programming. Instead of following predefined rules, ML models adapt by identifying relationships in datasets and making predictions or decisions accordingly.\n",
        "Main Components of Machine Learning\n",
        "Machine learning systems consist of several key components:\n",
        "- Data – The foundation of ML; includes structured or unstructured datasets used for training and evaluation.\n",
        "- Features – Measurable properties or characteristics extracted from data to improve model performance.\n",
        "- Model – The mathematical framework (e.g., neural networks, decision trees, or regression models) that maps input features to predictions.\n",
        "- Loss Function – A metric that quantifies errors or differences between predicted and actual values, helping improve model accuracy.\n",
        "- Optimization Algorithm – Techniques (such as gradient descent) that iteratively adjust model parameters to minimize the loss function.\n",
        "- Training Process – The phase where a model learns patterns from labeled data by optimizing parameters.\n",
        "- Validation & Testing – Steps used to evaluate model generalization and performance on unseen data.\n",
        "- Hyperparameters – Tunable parameters that influence training, such as learning rate, number of layers, or batch size.\n",
        "- Evaluation Metrics – Measures like accuracy, precision, recall, and F1-score to assess model performance.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**4.How does loss value help in determining whether the model is good or not?**\n",
        "Ans.Loss value, a numerical metric, is crucial in evaluating a model's performance by measuring the difference between its predictions and the actual target values. A lower loss value generally indicates a better-performing model, as it means the model's predictions are closer to the true values. The goal during model training is to minimize the loss, guiding the model to make better predictions.  Loss Helps Determine Model Quality:\n",
        "Training Guidance:\n",
        "Loss functions are used to guide the model during the training process.\n",
        "Parameter Adjustment:\n",
        "The model adjusts its internal parameters (like weights) to minimize the loss, leading to improved predictions.\n",
        "Performance Evaluation:\n",
        "By tracking the loss, you can see how the model's performance improves over time.\n",
        "Model Comparison:\n",
        "You can compare the loss values of different models to determine which one performs better.\n",
        "3. Loss and Accuracy:\n",
        "Inverse Relationship: Lower loss usually means higher accuracy (and vice versa).\n",
        "Not Always Perfect Correlation: While they are related, there can be situations where accuracy and loss don't perfectly align, especially in complex scenarios.\n",
        "4. Loss Curves:\n",
        "Visualizing Training:\n",
        "Loss curves (or learning curves) visually represent the change in loss over time during training.\n",
        "Identifying Problems:\n",
        "They can help identify issues like overfitting (where the model performs well on training data but poorly on new data) or underfitting (where the model hasn't learned enough from the data).\n",
        "\n",
        "\n",
        "\n",
        "**5.What are continuous and categorical variables?**\n",
        "Ans.In statistics and data analysis, continuous variables are those that can take on any value within a given range, while categorical variables represent distinct groups or categories. Continuous variables are numerical and can be measured, while categorical variables are descriptive.\n",
        "Continuous Variables:\n",
        "Definition:\n",
        "Continuous variables have an infinite number of possible values within a specified range.\n",
        "Examples:\n",
        "Height, weight, temperature, blood pressure, time, and age are all examples of continuous variables.\n",
        "Measurement:\n",
        "Continuous variables are typically measured using instruments or scales, allowing for precise numerical values.\n",
        "Categorical Variables:\n",
        "Definition:\n",
        "Categorical variables represent distinct groups or categories, and each observation belongs to only one of these categories.\n",
        "Examples:\n",
        "Gender (male, female), blood type (A, B, AB, O), race, and education level (high school, college, graduate).\n",
        "Categories:\n",
        "Categorical variables can have a limited number of possible values, often represented as labels or names.\n",
        "\n",
        "\n",
        "\n",
        "**6.How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        "Ans.To handle categorical variables in machine learning, several techniques can be used, including one-hot encoding, label encoding, ordinal encoding, target encoding, and frequency encoding. The choice of technique depends on the nature of the categorical variable (nominal or ordinal) and the specific requirements of the machine learning algorithm.\n",
        "Here's a breakdown of common techniques:\n",
        "1. One-Hot Encoding:\n",
        "Purpose:\n",
        "Converts categorical variables into a numerical representation by creating a binary column (0 or 1) for each category.\n",
        "When to use:\n",
        "For nominal categorical variables where categories do not have a natural order.\n",
        "Example:\n",
        "If you have a feature \"Color\" with categories \"Red,\" \"Blue,\" and \"Green,\" one-hot encoding would create three new columns: \"Color_Red,\" \"Color_Blue,\" and \"Color_Green,\" with 1 indicating the presence of that color.\n",
        "2. Label Encoding:\n",
        "Purpose:\n",
        "Assigns a unique integer to each category, effectively transforming the categorical variable into an ordinal variable.\n",
        "When to use:\n",
        "For ordinal categorical variables where the order of categories is meaningful.\n",
        "Example:\n",
        "If you have a feature \"Education Level\" with categories \"High School,\" \"Bachelor's,\" and \"Master's,\" you could label encode them as 1, 2, and 3, respectively, preserving the order.\n",
        "3. Ordinal Encoding:\n",
        "Purpose:\n",
        "Similar to label encoding, but specifically designed for ordinal variables, preserving the order of categories.\n",
        "When to use:\n",
        "For ordinal categorical variables where the order of categories is important.\n",
        "Example:\n",
        "You could use ordinal encoding to map \"Small,\" \"Medium,\" and \"Large\" to 1, 2, and 3, maintaining the order.\n",
        "4. Target Encoding (Mean Encoding):\n",
        "Purpose:\n",
        "Replaces each category with the average value of the target variable for that category.\n",
        "When to use:\n",
        "Can be effective when the target variable has a strong relationship with the categorical variable.\n",
        "Example:\n",
        "If you're predicting house prices, you could replace a region category with the average house price in that region.\n",
        "5. Frequency Encoding:\n",
        "Purpose:\n",
        "Replaces each category with its frequency or occurrence count in the dataset.\n",
        "When to use:\n",
        "Useful for handling high-cardinality categorical variables (variables with many unique categories) or when the frequency of categories is informative.\n",
        "Example:\n",
        "You could replace a city category with the number of times that city appears in the dataset.\n",
        "6. Binary Encoding:\n",
        "Purpose:\n",
        "Converts each category into its binary representation and then splits the binary digits into separate columns.\n",
        "When to use:\n",
        "Can reduce the number of columns compared to one-hot encoding, especially for high-cardinality variables.\n",
        "Example:\n",
        "If you have categories \"A,\" \"B,\" \"C,\" \"D\" which can be encoded as 00, 01, 10, 11 respectively, then you would have two columns, \"Binary_1\" and \"Binary_2\".\n",
        "Choosing the right technique:\n",
        "Nominal data: Use one-hot encoding or frequency encoding.\n",
        "Ordinal data: Use label encoding or ordinal encoding.\n",
        "High cardinality: Use target encoding, frequency encoding, or binary encoding.\n",
        "Avoiding overfitting: Be cautious with target encoding and consider cross-validation.\n",
        "In addition to these techniques, other methods like feature hashing, grid target encoding, and effect coding are also used, according to some sources and other articles.\n",
        "\n",
        "\n",
        "\n",
        "**7.What do you mean by training and testing a dataset?**\n",
        "Ans.In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data.Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided into multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation, and test sets.\n",
        "\n",
        "The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\n",
        "\n",
        "Successively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation data sets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set). This simple procedure is complicated in practice by the fact that the validation data set's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.\n",
        "\n",
        "Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set.[5] If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term \"validation set\" is sometimes used instead of \"test set\" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).\n",
        "\n",
        "Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**8.What is sklearn.preprocessing?**\n",
        "Ans.Key Functions in sklearn.preprocessing\n",
        "- Scaling & Normalization\n",
        "- StandardScaler(): Standardizes features (zero mean, unit variance).\n",
        "- MinMaxScaler(): Rescales features to a fixed range (e.g., [0,1]).\n",
        "- RobustScaler(): Uses median and IQR for scaling, effective against outliers.\n",
        "- Encoding Categorical Variables\n",
        "- LabelEncoder(): Converts categorical labels into numeric values.\n",
        "- OneHotEncoder(): Creates binary columns for categorical data (useful for ML models).\n",
        "- Generating Polynomial Features\n",
        "- PolynomialFeatures(): Expands features into polynomial terms (useful for nonlinear models).\n",
        "- Handling Missing Values\n",
        "- SimpleImputer(): Fills missing values using strategies like mean, median, or mode.\n",
        "- Binarization\n",
        "- Binarizer(): Converts numerical data into binary values based on a threshold.\n",
        "Since you're proficient in Python and data preprocessing, would you like a code snippet demonstrating how to scale and encode a dataset using sklearn.preprocessing.\n",
        "\n",
        "\n",
        "**9.What is a Test set?**\n",
        "Ans.A test set is a portion of data that is held back from the training process and used to evaluate the performance of a model after it has been trained. It's a crucial component of model development, ensuring that the model's ability to generalize to new, unseen data is accurately assessed.\n",
        "\n",
        "\n",
        "**10.How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "Ans.Splitting your dataset is essential for an unbiased evaluation of prediction performance. In most cases, it’s enough to split your dataset randomly into three subsets:\n",
        "\n",
        "The training set is applied to train or fit your model. For example, you use the training set to find the optimal weights, or coefficients, for linear regression, logistic regression, or neural networks.\n",
        "\n",
        "The validation set is used for unbiased model evaluation during hyperparameter tuning. For example, when you want to find the optimal number of neurons in a neural network or the best kernel for a support vector machine, you experiment with different values. For each considered setting of hyperparameters, you fit the model with the training set and assess its performance with the validation set.\n",
        "\n",
        "The test set is needed for an unbiased evaluation of the final model. You shouldn’t use it for fitting or validation.\n",
        "\n",
        "In less complex cases, when you don’t have to tune hyperparameters, it’s okay to work with only the training and test sets.\n",
        "\n",
        "Underfitting and Overfitting\n",
        "Splitting a dataset might also be important for detecting if your model suffers from one of two very common problems, called underfitting and overfitting:\n",
        "\n",
        "Underfitting is usually the consequence of a model being unable to encapsulate the relations among data. For example, this can happen when trying to represent nonlinear relations with a linear model. Underfitted models will likely have poor performance with both training and test sets.\n",
        "\n",
        "Overfitting usually takes place when a model has an excessively complex structure and learns both the existing relations among data and noise. Such models often have bad generalization capabilities. Although they work well with training data, they usually yield poor performance with unseen test data.train_test_split() is a function in sklearn that divides datasets into training and testing subsets.\n",
        "x_train and y_train represent the inputs and outputs of the training data subset, respectively, while x_test and y_test represent the input and output of the testing data subset.\n",
        "By specifying test_size=0.2, you use 20% of the dataset for testing, leaving 80% for training.\n",
        "train_test_split() can handle imbalanced datasets using the stratify parameter to maintain class distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**11.Why do we have to perform EDA before fitting a model to the data?**\n",
        "Ans.Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons. It helps uncover patterns, identify anomalies, and test hypotheses, ultimately leading to better model selection, feature engineering, and evaluation. EDA allows you to understand the data's characteristics and structure before building a model, which can prevent data leakage and ensure a more accurate assessment of model performance.\n",
        "\n",
        "\n",
        "\n",
        "**12.13. Same as question 2.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4_BAL4Gi7jx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#14.How can you find correlation between variables in Python?\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "x = np.array([10, 20, 30, 40, 50])\n",
        "y = np.array([5, 15, 25, 35, 45])\n",
        "\n",
        "# Compute correlation\n",
        "correlation_matrix = np.corrcoef(x, y)\n",
        "print(\"Correlation coefficient:\", correlation_matrix[0, 1])  # Extracting correlation value\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f4fQacOcIVN",
        "outputId": "1e06d333-e7c3-4870-ce5d-59e2ebf68ba0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation coefficient: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**15.What is causation? Explain difference between correlation and causation with an example.**\n",
        "Ans.Causation means that one event is the direct result of another. Correlation, on the other hand, means that two events are related, but one doesn't necessarily cause the other. For example, while there might be a correlation between eating ice cream and getting sunburned (both events are more common in the summer), eating ice cream doesn't directly cause sunburns. Instead, both are influenced by the underlying cause: sunny weather.\n",
        "\n",
        "\n",
        "**16.What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "Ans.An optimizer is an algorithm that adjusts model parameters (like weights and biases) to minimize the loss function during training, aiming to improve prediction accuracy. Different types of optimizers employ various strategies to converge towards optimal parameter values.\n",
        "Types of Optimizers:\n",
        "Gradient Descent (GD):\n",
        "This is a fundamental optimization algorithm that iteratively adjusts parameters in the direction of the negative gradient of the loss function. It's like climbing down a hill to find the lowest point.\n",
        "Example: In a linear regression model, GD adjusts the weights of the regression line to minimize the squared difference between predicted and actual values.\n",
        "Stochastic Gradient Descent (SGD):\n",
        "A variation of GD where parameters are updated after each training example (not a batch). This leads to more frequent updates and can sometimes escape local minima.\n",
        "Example: In a neural network, SGD updates weights after processing each individual image in the training set.\n",
        "Momentum:\n",
        "Adds a momentum term to SGD, simulating inertia. This helps the algorithm move faster through flat regions of the loss landscape and converge more quickly.\n",
        "Example: Similar to SGD, but the update is influenced by the previous update's direction, leading to smoother convergence.\n",
        "Adagrad:\n",
        "Adapts the learning rate for each parameter individually based on the historical gradients. Parameters with larger historical gradients get smaller learning rates, and vice versa.\n",
        "Example: In a neural network, Adagrad might adjust the learning rate for a particular layer more rapidly if its gradients are consistently large.\n",
        "RMSprop:\n",
        "Similar to Adagrad, but it uses a moving average of squared gradients to adapt the learning rate. It is more stable than Adagrad, especially when dealing with noisy gradients.\n",
        "Example: In a CNN, RMSprop might adjust the learning rate for certain convolutional layers more rapidly if their gradients are consistently high.\n",
        "Adam:\n",
        "Combines the advantages of both Adagrad and RMSprop, using both the first and second moments of the gradients to adapt the learning rate. It's a popular and often effective choice for many tasks.\n",
        "Example: In a recurrent neural network (RNN), Adam might adjust the learning rate for each layer based on a combination of historical gradients and squared gradients.\n",
        "\n",
        "\n",
        "**17.What is sklearn.linear_model ?**\n",
        "Ans.linear_model is a class of the sklearn module if contain different functions for performing machine learning with linear models. The term linear model implies that the model is specified as a linear combination of features.\n",
        "\n",
        "\n",
        "\n",
        "**18.What does model.fit() do? What arguments must be given?**\n",
        "Ans.In TensorFlow,model.fit() function is used to train a machine learning model for a fixed number of epochs (iterations over the entire dataset). During training, the model adjusts its internal parameters (weights and biases) to minimize the loss function using optimization techniques like Gradient Descent.Arguments are included in the function.\n",
        "model.fit(\n",
        "    x=None,\n",
        "    y=None,\n",
        "    batch_size=None,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    validation_data=None,\n",
        "    validation_split=0.0,\n",
        "    callbacks=None,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "Parameters:\n",
        "\n",
        "x (input data): The input data for training. This can be a NumPy array, TensorFlow dataset, or any other valid tensor-like object.\n",
        "y (target data): The labels or target data corresponding to the input data x.\n",
        "batch_size: Number of samples per gradient update. It determines the size of each mini-batch for training.\n",
        "epochs: The number of times to iterate over the entire dataset. This defines how many times the model will learn from the data.\n",
        "verbose: Controls the verbosity of the training process:\n",
        "0: No output.\n",
        "1: Progress bar.\n",
        "2: One line per epoch.\n",
        "validation_data: Data used for evaluating the model performance during training, typically a tuple (x_val, y_val).\n",
        "validation_split: Fraction of the training data to be used for validation (e.g., 0.2 means 20% of data will be used for validation).\n",
        "callbacks: A list of callback functions that are executed at various stages of training, such as EarlyStopping or ModelCheckpoint.\n",
        "shuffle: Whether to shuffle the training data before each epoch to improve generalization.\n",
        "\n",
        "\n",
        "\n",
        "**19.What does model.predict() do? What arguments must be given?**\n",
        "Ans.Purpose: model.predict() is used to generate predictions from the trained model based on new input data. It does not require true labels and does not compute any metrics.\n",
        "Use Case: This function is utilized when you want to obtain the model's predictions for new or unseen data, typically for tasks such as classification, regression, or any other type of prediction task.\n",
        "Working: It takes input data and feeds it through the model to generate predictions. The output depends on the nature of the task (e.g., probabilities for classification tasks, continuous values for regression tasks).\n",
        "Output: The output of model.predict() is the predicted labels or values for the input data. The format of the output will match the type of model (e.g., a classification model might return a vector of probabilities).\n",
        "When to Use: Use model.predict() when you want to make predictions on new data and obtain the model's outputs without calculating any loss or metrics.\n",
        "The predict() function accepts only a single argument which is usually the data to be tested. It returns the labels of the data passed as argument based upon the learned or trained data obtained from the model.\n",
        "\n",
        "\n",
        "\n",
        "**20.What are continuous and categorical variables?**\n",
        "Ans.In statistics, continuous variables are measurable on a continuous scale, meaning they can take any value within a given range. Categorical variables represent groups or categories and can only take on a fixed number of values.\n",
        "\n",
        "\n",
        "**21.What is feature scaling? How does it help in Machine Learning?**\n",
        "Ans.Feature scaling is a crucial data preprocessing step in machine learning that transforms numerical features to a common scale, ensuring they contribute equally to the model's learning process. This is particularly important for algorithms that are sensitive to the scale of input data, such as gradient descent-based methods and those relying on distance calculations.\n",
        "Here's how feature scaling helps:\n",
        "Improved Model Performance:\n",
        "By scaling features to a similar range, you prevent features with larger magnitudes from dominating the model's learning process. This can lead to more accurate and reliable predictions.\n",
        "Faster Convergence:\n",
        "Many machine learning algorithms, especially those using gradient descent, converge faster when features are scaled. This means the model learns more efficiently and requires fewer iterations to reach an optimal solution.\n",
        "Equal Feature Contribution:\n",
        "Feature scaling ensures that all features contribute equally to the model's learning process, preventing any single feature from having an undue influence.\n",
        "Better Interpretability:\n",
        "When features are scaled, it becomes easier to compare the importance of different features, especially in linear regression or tree-based models.\n",
        "Enhanced Model Stability:\n",
        "Scaling can help stabilize the model's performance, especially when dealing with noisy or poorly-behaved data.\n",
        "Common feature scaling techniques include:\n",
        "Normalization: Rescaling features to a range between 0 and 1.\n",
        "Standardization: Transforming features to have a mean of 0 and a standard deviation of 1.\n",
        "Min-Max Scaling: Similar to normalization, but with a specific range.\n",
        "For example, in a dataset with \"age\" and \"annual income,\" \"annual income\" might have a much larger range than \"age.\" Without scaling, the \"annual income\" feature could disproportionately influence algorithms that rely on distance calculations, such as k-nearest neighbors or support vector machines. Feature scaling ensures that both features contribute equally to the model's calculations, leading to more accurate and reliable results.\n"
      ],
      "metadata": {
        "id": "9nWbgKWbcoDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#22.How do we perform scaling in Python?\n",
        "#Standardisation/Z Score Tailing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[100, 200], [300, 400], [500, 600]])\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm3fTbBymOll",
        "outputId": "13edb3c5-9334-4843-cf13-e97bccfa15c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "r5b-hjsScoFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Min-Max Scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veglZgHym7GB",
        "outputId": "538f6d6e-66d8-459d-9fbb-6bd10d1a3358"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Robust Scaling\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcTzlQ7lnFPv",
        "outputId": "25bc8b9b-68e0-4a2b-f06c-a6590192745d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1. -1.]\n",
            " [ 0.  0.]\n",
            " [ 1.  1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization(L1 or L2 norm)\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "scaler = Normalizer()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoE-XEBXnTRq",
        "outputId": "32ee7aec-550f-413b-a188-9670d3e0f533"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4472136  0.89442719]\n",
            " [0.6        0.8       ]\n",
            " [0.6401844  0.76822128]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**23.What is sklearn.preprocessing?**\n",
        "Ans.The sklearn. preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set.\n",
        "\n",
        "\n",
        "**24.How do we split data for model fitting (training and testing) in Python?**\n",
        "Ans."
      ],
      "metadata": {
        "id": "3EK2qj6xnpLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data into a DataFrame (replace with your actual data)\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [6, 7, 8, 9, 10],\n",
        "    'target': [11, 12, 13, 14, 15]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now you can use X_train, y_train to train your model and X_test, y_test to test it\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9N81zyFoSqF",
        "outputId": "642fd03e-de20-44d1-a342-9127e2ccff52"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (4, 2)\n",
            "Shape of X_test: (1, 2)\n",
            "Shape of y_train: (4,)\n",
            "Shape of y_test: (1,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**25.Explain data encoding?**\n",
        "Ans.Data encoding is the process of converting information into a specific format, often a digital one, to enable efficient transmission, storage, and manipulation by computers and other devices. It essentially acts as a bridge between raw digital information and human-readable or machine-understandable data. This process can involve converting text into binary, representing images as pixel data, or transforming data into formats suitable for machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "VM20HTv5oY0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kAalvmmHoY3F"
      }
    }
  ]
}